{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.utils\n",
    "import nlp_util\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Conv1D, Dense, Input, Lambda, LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import _pickle as cPickle\n",
    "\n",
    "from keras.layers import Concatenate, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, MaxPool1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataset\n",
    "train_file=pd.read_csv('train.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "test_file=pd.read_csv('test.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "va_file=pd.read_csv('valid.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "\n",
    "column_names = ['Id', 'Label','Statement','Subject','Speaker','Speaker Job','State Info','Party','BT','FC','HT','MT','PF','Context']\n",
    "train_file.columns, test_file.columns, va_file.columns=column_names, column_names, column_names\n",
    "\n",
    "train_data = train_file[train_file.columns[~train_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]\n",
    "test_data = test_file[test_file.columns[~test_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]\n",
    "val_data = va_file[va_file.columns[~va_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_labels_dict = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "\n",
    "def one_hot_label(label):\n",
    "    return to_categorical(multi_labels_dict[x], num_classes=6)\n",
    "\n",
    "train_data['multi_label']=train_data['Label'].apply(lambda x: multi_labels_dict[x])\n",
    "test_data['multi_label']=test_data['Label'].apply(lambda x: multi_labels_dict[x])\n",
    "val_data['multi_label']=val_data['Label'].apply(lambda x: multi_labels_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers= ['barack-obama', 'donald-trump', 'hillary-clinton', 'mitt-romney', \n",
    "                'scott-walker', 'john-mccain', 'rick-perry', 'chain-email', \n",
    "                'marco-rubio', 'rick-scott', 'ted-cruz', 'bernie-s', 'chris-christie', \n",
    "                'facebook-posts', 'charlie-crist', 'newt-gingrich', 'jeb-bush', \n",
    "                'joe-biden', 'blog-posting','paul-ryan']\n",
    "\n",
    "speaker_dict={}\n",
    "for cnt, speaker in enumerate(speakers):\n",
    "    speaker_dict[speaker]=cnt\n",
    "print(speaker_dict)\n",
    "\n",
    "def speaker_projection(speaker):\n",
    "    if isinstance(speaker, str):\n",
    "        speaker=speaker.lower()\n",
    "        matched=[s for s in speakers if s in speaker]\n",
    "        if len(matched)>0:\n",
    "            return speaker_dict[matched[0]]\n",
    "        else:\n",
    "            return len(speakers)\n",
    "        \n",
    "##\n",
    "train_data['speaker_id']=train_data['Speaker'].apply(speaker_projection)\n",
    "test_data['speaker_id']=test_data['Speaker'].apply(speaker_projection)\n",
    "val_data['speaker_id']=val_data['Speaker'].apply(speaker_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map job\n",
    "job_list = ['president', 'u.s. senator', 'governor', 'president-elect', 'presidential candidate', \n",
    "                'u.s. representative', 'state senator', 'attorney', 'state representative', 'congress', 'others']\n",
    "\n",
    "\n",
    "job_dict = {'president':0, 'u.s. senator':1, 'governor':2, 'president-elect':3, 'presidential candidate':4, \n",
    "            'u.s. representative':5, 'state senator':6, 'attorney':7, 'state representative':8, 'congress':9, 'others':10}\n",
    "\n",
    "## Map job\n",
    "\n",
    "def job_projection(job):\n",
    "    if isinstance(job, str):\n",
    "        job=job.lower()\n",
    "        matched_job=[j for j in job_list if j in job]\n",
    "        if len(matched_job)>0:\n",
    "            return job_dict[matched_job[0]]\n",
    "        else:\n",
    "            return 10\n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "## job projection output\n",
    "\n",
    "train_data['job_id']=train_data['Speaker Job'].apply(job_projection)\n",
    "test_data['job_id']=test_data['Speaker Job'].apply(job_projection)\n",
    "val_data['job_id']=val_data['Speaker Job'].apply(job_projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Map political parties\n",
    "party_dict={'republican':0, 'democrat':1, 'none':2, 'organization':3, 'newsmaker':4, 'rest':5}\n",
    "\n",
    "def map_political_party(party):\n",
    "    if party in party_dict:\n",
    "        return party_dict[party]\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "##\n",
    "train_data['party_id']=train_data['Party'].apply(map_political_party)\n",
    "test_data['party_id']=test_data['Party'].apply(map_political_party)\n",
    "val_data['party_id']=val_data['Party'].apply(map_political_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map states\n",
    "all_states = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado',\n",
    "              'Connecticut','Delaware','Florida','Georgia','Hawaii','Idaho', \n",
    "              'Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana',\n",
    "              'Maine' 'Maryland','Massachusetts','Michigan','Minnesota',\n",
    "              'Mississippi', 'Missouri','Montana','Nebraska','Nevada',\n",
    "              'New Hampshire','New Jersey','New Mexico','New York',\n",
    "              'North Carolina','North Dakota','Ohio',    \n",
    "              'Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "              'South  Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "              'Vermont','Virginia','Washington','West Virginia',\n",
    "              'Wisconsin','Wyoming']\n",
    "\n",
    "\n",
    "states_dict = {'wyoming': 48, 'colorado': 5, 'washington': 45, 'hawaii': 10, \n",
    "               'tennessee': 40, 'wisconsin': 47, 'nevada': 26, 'north dakota': 32, \n",
    "               'mississippi': 22, 'south dakota': 39, 'new jersey': 28, 'oklahoma': 34, \n",
    "               'delaware': 7, 'minnesota': 21, 'north carolina': 31, 'illinois': 12, \n",
    "               'new york': 30, 'arkansas': 3, 'west virginia': 46, 'indiana': 13, \n",
    "               'louisiana': 17, 'idaho': 11, 'south  carolina': 38, 'arizona': 2, \n",
    "               'iowa': 14, 'mainemaryland': 18, 'michigan': 20, 'kansas': 15, \n",
    "               'utah': 42, 'virginia': 44, 'oregon': 35, 'connecticut': 6, 'montana': 24, \n",
    "               'california': 4, 'massachusetts': 19, 'rhode island': 37, 'vermont': 43, \n",
    "               'georgia': 9, 'pennsylvania': 36, 'florida': 8, 'alaska': 1, 'kentucky': 16,\n",
    "               'nebraska': 25, 'new hampshire': 27, 'texas': 41, 'missouri': 23, 'ohio': 33,\n",
    "               'alabama': 0, 'new mexico': 29, 'rest':50}\n",
    "\n",
    "\n",
    "def state_projection(state):\n",
    "    if isinstance(state, str):\n",
    "        state=state.lower()\n",
    "        if state in states_dict:\n",
    "            return states_dict[state]\n",
    "        else:\n",
    "            if 'washington' in state:\n",
    "                return states_dict['washington']\n",
    "            else:\n",
    "                return 50\n",
    "    else:\n",
    "        return 50\n",
    "    \n",
    "## state mapping output:\n",
    "train_data['state_id']=train_data['State Info'].apply(state_projection)\n",
    "test_data['state_id']=test_data['State Info'].apply(state_projection)\n",
    "val_data['state_id']=val_data['State Info'].apply(state_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map subject\n",
    "subject_list = ['health','tax','immigration','election','education',\n",
    "    'candidates-biography','economy','gun','jobs','federal-budget','energy','abortion','foreign-policy']\n",
    "\n",
    "subject_dict = {'health':0,'tax':1,'immigration':2,'election':3,'education':4,\n",
    "                'candidates-biography':5,'economy':6,'gun':7,'jobs':8,'federal-budget':9,\n",
    "                'energy':10,'abortion':11,'foreign-policy':12, 'others':13}\n",
    "\n",
    "## mapping subject\n",
    "def subject_projection(subject):\n",
    "    if isinstance(subject, str):\n",
    "        subject=subject.lower()\n",
    "        matched_subject=[subj for subj in subject_list if subj in subject]\n",
    "        \n",
    "        if len(matched_subject)>0:\n",
    "            return subject_dict[matched_subject[0]]\n",
    "        else:\n",
    "            return 13\n",
    "    else:\n",
    "        return 13\n",
    "    \n",
    "##\n",
    "train_data['subject_id']=train_data['Subject'].apply(subject_projection)\n",
    "test_data['subject_id']=test_data['Subject'].apply(subject_projection)\n",
    "val_data['subject_id']=val_data['Subject'].apply(subject_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context mapping\n",
    "Context_list = ['news release','interview','tv','radio',\n",
    "                'campaign','news conference','press conference','press release',\n",
    "                'tweet','facebook','email']\n",
    "\n",
    "Context_dict = {'news release':0,'interview':1,'tv':2,'radio':3,\n",
    "                'campaign':4,'news conference':5,'press conference':6,'press release':7,\n",
    "                'tweet':8,'facebook':9,'email':10, 'others':11}\n",
    "\n",
    "def Context_projection(context):\n",
    "    if isinstance(context, str):\n",
    "        context=context.lower()\n",
    "        matched_context=[cntx for cntx in Context_list if cntx in context]\n",
    "        if len(matched_context)>0:\n",
    "            return Context_dict[matched_context[0]]\n",
    "        else:\n",
    "            return 11\n",
    "    else:\n",
    "        return 11\n",
    "    \n",
    "## context projection output\n",
    "train_data['context_id']=train_data['Context'].apply(Context_projection)\n",
    "test_data['context_id']=test_data['Context'].apply(Context_projection)\n",
    "val_data['context_id']=val_data['Context'].apply(Context_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenize fake news statement and build vocabulary\n",
    "vocab_dict={}\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_data['Statement'])\n",
    "vocab_dict=tokenizer.word_index\n",
    "cPickle.dump(tokenizer.word_index, open(\"vocab.p\",\"wb\"))\n",
    "print(\"vocab dictionary is created\")\n",
    "print(\"saved vocan dictionary to pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preprocessing\n",
    "\n",
    "# def preprocessing_txt(dataset):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     corpus=[]\n",
    "#     for elm in range(0, len(dataset.index)):\n",
    "#         res=' '.join([i for i in dataset['Statement'][elm].lower().split() if i not in stop_words])\n",
    "#         res=re.sub(\"</?.*?>\",\" <> \",dataset['Statement'][elm])    # remove tags\n",
    "#         res=re.sub(\"(\\\\d|\\\\W)+\",\" \",dataset['Statement'][elm])        # remove special characte\n",
    "#         res=re.sub(r'@([A-Za-z0-9_]+)', \"\",dataset['Statement'][elm])  # remove twitter handler\n",
    "#         res=re.sub('(\\r)+', \"\", dataset['Statement'][elm])            # remove newline character\n",
    "#         res=re.sub('[^\\x00-\\x7F]+', \"\", dataset['Statement'][elm])    # remove non-ascii characters\n",
    "#         res=''.join(x for x in dataset['Statement'][elm] if x not in set(string.punctuation))   ## remove punctuation\n",
    "#         corpus.append(res)\n",
    "#     return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab_dict.keys())\n",
    "hidden_size = 100 #Has to be same as EMBEDDING_DIM\n",
    "lstm_size = 100\n",
    "num_steps = 64\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [2,3,5,7]\n",
    "filter_size = 128\n",
    "#Meta data related hyper params\n",
    "num_party = 6\n",
    "num_state = 51\n",
    "num_context = 12\n",
    "num_job = 11\n",
    "num_sub = 14\n",
    "num_speaker = 21\n",
    "embedding_dims=300\n",
    "max_features = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create embedding layer\n",
    "num_words=len(vocab_dict)+1\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "glove_model = loadGloveModel('glove.6B.300d.txt')\n",
    "\n",
    "def build_glove_embedding_layers():\n",
    "    embed_matrix=np.zeros((max_features, embedding_dims))\n",
    "    for word, indx in tokenizer.word_index.items():\n",
    "        if indx >= max_features:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return embed_matrix\n",
    "\n",
    "embedding_weights=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data preprocessing\n",
    "def preprocessing_txt_keras(statement):\n",
    "    txt=text_to_word_sequence(statement)\n",
    "    val=[0]*20\n",
    "    val=[vocab_dict[t] for t in txt if t in vocab_dict]   ##replace unknown words with zero index\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training instances list\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "### remove stopwords fitst\n",
    "train_data['Statement'] = list(map(' '.join, train_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "test_data['Statement'] = list(map(' '.join, test_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "val_data['Statement'] = list(map(' '.join, val_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['word_id']=train_data['Statement'].apply(preprocessing_txt_keras)\n",
    "test_data['word_id']=test_data['Statement'].apply(preprocessing_txt_keras)\n",
    "val_data['word_id']=val_data['Statement'].apply(preprocessing_txt_keras)\n",
    "\n",
    "x_train=train_data['word_id']\n",
    "x_test=test_data['word_id']\n",
    "x_val=val_data['word_id']\n",
    "\n",
    "y_train=train_data['multi_label']\n",
    "y_val=val_data['multi_label']\n",
    "\n",
    "##\n",
    "x_train=sequence.pad_sequences(x_train, maxlen=num_steps, padding='post', truncating='post')\n",
    "y_train=to_categorical(y_train, num_classes=6)\n",
    "x_val=sequence.pad_sequences(x_val, maxlen=num_steps, padding='post', truncating='post')\n",
    "y_val=to_categorical(y_val, num_classes=6)\n",
    "x_test=sequence.pad_sequences(x_test, maxlen=num_steps, padding='post', truncating='post')\n",
    "\n",
    "## meta data preparation\n",
    "tr_party=to_categorical(train_data['party_id'], num_classes=num_party)\n",
    "tr_state=to_categorical(train_data['state_id'], num_classes=num_state)\n",
    "tr_cont=to_categorical(train_data['context_id'], num_classes=num_context)\n",
    "tr_job=to_categorical(train_data['job_id'], num_classes=num_job)\n",
    "tr_subj=to_categorical(train_data['subject_id'], num_classes=num_sub)\n",
    "# tr_speaker=to_categorical(train_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "# x_train_metadata=np.hstack(tr_party, tr_state, tr_job, tr_subj, tr_speaker, tr_cont)\n",
    "x_train_metadata=np.hstack((tr_party, tr_state, tr_cont,tr_job, tr_subj))\n",
    "\n",
    "\n",
    "# #********************************************************************************#\n",
    "val_party=to_categorical(val_data['party_id'], num_classes=num_party)\n",
    "val_state=to_categorical(val_data['state_id'], num_classes=num_state)\n",
    "val_cont=to_categorical(val_data['context_id'], num_classes=num_context)\n",
    "val_job=to_categorical(val_data['job_id'], num_classes=num_job)\n",
    "val_subj=to_categorical(val_data['subject_id'], num_classes=num_sub)\n",
    "# val_speaker=to_categorical(val_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "# x_val_metadata=np.hstack(val_party, val_state, val_job, val_subj, val_speaker, val_cont)\n",
    "x_val_metadata=np.hstack((val_party, val_state, val_cont,val_job, val_subj, ))\n",
    "\n",
    "# #********************************************************************************#\n",
    "te_party=to_categorical(test_data['party_id'], num_classes=num_party)\n",
    "te_state=to_categorical(test_data['state_id'], num_classes=num_state)\n",
    "te_cont=to_categorical(test_data['context_id'], num_classes=num_context)\n",
    "te_job=to_categorical(test_data['job_id'], num_classes=num_job)\n",
    "te_subj=to_categorical(test_data['subject_id'], num_classes=num_sub)\n",
    "# te_speaker=to_categorical(test_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "x_test_metadata=np.hstack((te_party, te_state, te_cont,te_job, te_subj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to initialize model weight\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embed_sequences = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel, \n",
    "                 padding=\"same\", activation=\"relu\", strides=1)(embed_sequences)\n",
    "    x_1 = MaxPool1D(3)(x_1)\n",
    "    x_flat = Flatten()(x_1)\n",
    "    x_drop = Dropout(0.7)(x_flat)\n",
    "    kernel_arr.append(x_drop)\n",
    "\n",
    "conv_in = keras.layers.concatenate(kernel_arr)\n",
    "conv_in = Dropout(0.85)(conv_in)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_drop = Dropout(0.7)(meta_input)\n",
    "x_meta = Dense(100, activation='relu')(x_drop)\n",
    "x = keras.layers.concatenate([conv_in, x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "#************************************************************************#\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history= model.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                   {'main_output': y_train},epochs=30, batch_size=100,\n",
    "                   validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                  callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_curve=plt.figure()\n",
    "plt.plot(history.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : CNN',fontsize=12)\n",
    "accu_curve.savefig('accuracy_cnn_improved_v1.2.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :CNN',fontsize=12)\n",
    "loss_curve.savefig('loss_cnn_improved_v1.2.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use multi-convolutional CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do small modification on above CNN model, in this notebook I introduce multi convolutional CNN model for fake news classification. let's see how it works and see where is significancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_input = Input(shape=(num_steps,), dtype='int64', name='main_input')\n",
    "embed_sequence = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "\n",
    "## add multi convolutional layer\n",
    "l_conv1 = Conv1D(128,3,activation=\"relu\", padding=\"same\", strides=1)(embed_sequence)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128, 3, activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPooling1D(3)(l_conv2)\n",
    "# l_conv3 = Conv1D(128,3,activation=\"relu\")(l_pool2)\n",
    "# l_pool3 = MaxPool1D(3)(l_conv3)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "conv_in = Dropout(0.9)(l_flat)\n",
    "conv_in = Dense(hidden_size, activation='relu')(conv_in)\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_meta = Dense(64, activation='relu')(meta_input)\n",
    "x = keras.layers.concatenate([conv_in, x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model_multi = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "#************************************************************************#\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_multi.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_multi.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history22= model_multi.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                           {'main_output': y_train},epochs=15, batch_size=100,\n",
    "                           validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                           callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to see performance difference\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(history22.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history22.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : CNN_multi',fontsize=12)\n",
    "accu_curve.savefig('accuracy_multi_conv_cnn.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history22.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history22.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :CNN Multi',fontsize=12)\n",
    "loss_curve.savefig('loss_multi_conv_cnn.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if we run the model multiple times (Experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embed_sequences = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "# for kernel in kernel_sizes:\n",
    "#     x_1 = Conv1D(filters=filter_size,kernel_size=kernel, \n",
    "#                  padding=\"same\", activation=\"relu\", strides=1)(embed_sequences)\n",
    "#     x_1 = MaxPool1D(3)(x_1)\n",
    "#     x_flat = Flatten()(x_1)\n",
    "#     x_drop = Dropout(0.6)(x_flat)\n",
    "#     kernel_arr.append(x_drop)\n",
    "\n",
    "# conv_in = keras.layers.concatenate(kernel_arr)\n",
    "# conv_in = Dropout(0.7)(conv_in)\n",
    "# conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "# #Meta input\n",
    "# meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "# x_drop = Dropout(0.7)(meta_input)\n",
    "# x_meta = Dense(100, activation='relu')(x_drop)\n",
    "# x = keras.layers.concatenate([conv_in, x_meta])\n",
    "\n",
    "# main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "# model = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "## add multi convolutional layer\n",
    "l_conv1 = Conv1D(128,3,activation=\"relu\", padding=\"same\", strides=1)(embed_sequences)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128, 3, activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPooling1D(3)(l_conv2)\n",
    "# l_conv3 = Conv1D(128,3,activation=\"relu\")(l_pool2)\n",
    "# l_pool3 = MaxPool1D(3)(l_conv3)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "conv_in = Dropout(0.9)(l_flat)\n",
    "conv_in = Dense(hidden_size, activation='relu')(conv_in)\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_drop = Dropout(0.8)(meta_input)\n",
    "x_meta = Dense(64, activation='relu')(x_drop)\n",
    "x = keras.layers.concatenate([conv_in, x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "####\n",
    "runs=5\n",
    "histArr = []\n",
    "\n",
    "for i in range(runs):\n",
    "    print('Running iteration %i/%i' % (i+1, runs))\n",
    "    model_multirun = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "    model_multirun.compile(optimizer=sgd,\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['categorical_accuracy'])\n",
    "    \n",
    "    ####\n",
    "    tb = TensorBoard()\n",
    "    csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "    #es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "    #filepath= \"weights.best.hdf5\"\n",
    "    # checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "    #                                              monitor='val_categorical_accuracy', \n",
    "    #                                              verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    history22= model_multirun.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                                  {'main_output': y_train},epochs=30, batch_size=100, verbose=1,\n",
    "                                  validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                                  callbacks=[keras.callbacks.ModelCheckpoint('model-%i.h5'%(i+1), \n",
    "                                                                              monitor='val_categorical_accuracy', verbose=1, \n",
    "                                                                              save_best_only=True, mode='max'), tb, csv_logger])\n",
    "                       #callbacks=[tb,csv_logger,checkpoint, es])\n",
    "    print()\n",
    "    histArr.append(history22.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the output\n",
    "with open('history22.pkl', 'wb') as f:\n",
    "    cPickle.dump(histArr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histArr = cPickle.load(open('history22.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg(histArr, his_key):\n",
    "    tmp = []\n",
    "    for history in histArr:\n",
    "        tmp.append(history[his_key][np.argmin(history['val_loss'])])\n",
    "    return np.mean(tmp)\n",
    "    \n",
    "print('Training: \\t%0.4f loss / %0.4f acc' % (get_avg(histArr, 'loss'),\n",
    "                                              get_avg(histArr, 'categorical_accuracy')))\n",
    "print('Validation: \\t%0.4f loss / %0.4f acc' % (get_avg(histArr, 'val_loss'),\n",
    "                                                get_avg(histArr, 'val_categorical_accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(title, histArr, key_acc, key_loss):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    # Accuracy\n",
    "    ax1.set_title('Model accuracy (%s)' % title)\n",
    "    names = []\n",
    "    for i, model in enumerate(histArr):\n",
    "        ax1.plot(model[key_acc])\n",
    "        ax1.set_xlabel('epoch')\n",
    "        names.append('Model %i' % (i+1))\n",
    "        ax1.set_ylabel('accuracy')\n",
    "    ax1.legend(names, loc='lower right')\n",
    "    # Loss\n",
    "    ax2.set_title('Model loss (%s)' % title)\n",
    "    for model in histArr:\n",
    "        ax2.plot(model[key_loss])\n",
    "        ax2.set_xlabel('epoch')\n",
    "        ax2.set_ylabel('loss')\n",
    "    ax2.legend(names, loc='upper right')\n",
    "    fig.set_size_inches(20, 5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss('training', histArr, 'categorical_accuracy', 'loss')\n",
    "plot_acc_loss('validation', histArr, 'val_categorical_accuracy', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make prediction\n",
    "from keras.models import load_model\n",
    "#Load a pre-trained model if any\n",
    "model1 = load_model('weights.best.hdf5')\n",
    "preds = model1.predict([x_test,x_test_metadata], batch_size=batch_size, verbose=1)\n",
    "print(np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils.np_utils import accuracy\n",
    "# acc = accuracy(y_test, np.round(np.array(preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  C-LSTM model for fake news classification\n",
    "\n",
    "To better understand performance gap between different convolutional models, here I want to see Convolutional -LSTM model for fake news classification. Note that I didn't run multiple convolutional filers for fake news texts because it raise strange error, so I'll proceed somehow simple but powerful model for fake news classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C-LSTM model implementation (trail version)\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "statement_input = Input(shape=(num_steps,), dtype='int64', name='main_input')\n",
    "# embed_layer = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "embed_layer = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "l_lstm = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(embed_layer)\n",
    "\n",
    "## let's add convolutional layer on it\n",
    "l_conv1 = Conv1D(128, 3, activation=\"relu\")(l_lstm)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128,3,activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPool1D(3)(l_conv2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "\n",
    "# l_pool = GlobalMaxPool1D()(l_lstm)\n",
    "l_drop = Dropout(0.6)(l_flat)\n",
    "conv_in = Dense(hidden_size, activation=\"relu\")(l_drop)\n",
    "\n",
    "### metadata\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_meta_hidden = Dense(64, activation='relu')(meta_input)\n",
    "x_meta_reg = Dropout(0.6)(x_meta_hidden)\n",
    "\n",
    "conv_final = keras.layers.concatenate([conv_in, x_meta_reg])\n",
    "model_output = Dense(6, activation='softmax', name='main_output')(conv_final)\n",
    "model_CLSTM = Model(inputs=[statement_input, meta_input], outputs=[model_output])\n",
    "\n",
    "## compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_CLSTM.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_CLSTM.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history_clstm = model_CLSTM.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                                {'main_output': y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "                                validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                                callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize c-lstm model output\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(history_clstm.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history_clstm.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : C-LSTM',fontsize=12)\n",
    "accu_curve.savefig('accuracy_clstm_improved_v1.4.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history_clstm.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history_clstm.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :C-LSTM',fontsize=12)\n",
    "loss_curve.savefig('loss_clstm_improved_v1.4.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Improvised version of C-LSTM model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++##\n",
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embed_sequences = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel, \n",
    "                 padding=\"same\", activation=\"relu\", strides=1)(embed_sequences)\n",
    "    x_pool = MaxPool1D(2)(x_1)\n",
    "    #x_pool = GlobalMaxPool1D()(x_1)\n",
    "    x_flat = Flatten()(x_pool)\n",
    "    #x_drop = Dropout(0.7)(x_flat)\n",
    "    kernel_arr.append(x_flat)\n",
    "\n",
    "conv_in = keras.layers.concatenate(kernel_arr)\n",
    "conv_in = Dropout(0.85)(conv_in)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "## use bidirectional lstm\n",
    "x_lstm = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x_1)\n",
    "lstm_pool = GlobalMaxPool1D()(x_lstm)\n",
    "lstm_dense = Dense(100, activation=\"relu\")(lstm_pool)\n",
    "\n",
    "## first concatenate\n",
    "conv_ins= keras.layers.concatenate([conv_in, lstm_dense])\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_drop = Dropout(0.7)(meta_input)\n",
    "x_meta = Dense(100, activation='relu')(x_drop)\n",
    "x = keras.layers.concatenate([conv_ins,x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model_clstm = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "## compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_clstm.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model_clstm.summary()\n",
    "\n",
    "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from math import sqrt\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Concatenate, Embedding, Flatten\n",
    "from keras.layers import AlphaDropout\n",
    "from keras.layers import ThresholdedReLU\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### character level classification (simple version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers=[[256, 7, 3],\n",
    "             [256, 7, 3],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers=[1024, 1024]\n",
    "dropout_p=0.5\n",
    "threshold=1e-06\n",
    "input_size=70\n",
    "alphabet='abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "\n",
    "batch_size=128\n",
    "checkpoint_every=70\n",
    "epochs=5000\n",
    "# evaluate_every=100\n",
    "batch_size=128\n",
    "checkpoint_every=100\n",
    "epochs=5000\n",
    "# evaluate_every=100\n",
    "embedding_size=128\n",
    "alphabet_size=69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## define model input, create embedding layer\n",
    "sent_inputs = Input(shape=(input_size,), name='sent_input', dtype='int64')  # shape=(?, 1014)\n",
    "\n",
    "conv = Embedding(alphabet_size+1, embedding_size, input_length=input_size, trainable=False)(sent_inputs)\n",
    "# Conv \n",
    "conv_lay=[]\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x_conv = Conv1D(filter_num, filter_size, padding=\"same\")(conv) \n",
    "    x_pool  = GlobalMaxPooling1D()(x_conv)\n",
    "    x_drop = Dropout(0.6)(x_pool)\n",
    "    conv_lay.append(x_drop)\n",
    "#     if pooling_size != -1:\n",
    "#         conv = MaxPooling1D(pool_size=pooling_size)(conv) # Final shape=(None, 34, 256)\n",
    "\n",
    "# x = Flatten()(conv_lay) # (None, 8704)\n",
    "x=concatenate(conv_lay)\n",
    "\n",
    "# Fully connected layers \n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x) # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "    \n",
    "model_output = Dense(6, activation='softmax')(x)\n",
    "# Build model\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "\n",
    "model_char = Model(inputs=sent_inputs, outputs=model_output)\n",
    "model_char.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "model_char.summary()\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "es=EarlyStopping(monitor=\"val_loss\", mode='min', verbose=1, patience=3 )\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "# history_char= model_char.fit(x_train,y_train,epochs=num_epochs, batch_size=batch_size,\n",
    "#                                validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_char.fit(x_train,y_train,epochs=10, batch_size=64, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recurrent CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jshayi2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "C:\\Users\\jshayi2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\jshayi2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\jshayi2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 300)    150000300   input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, None, 200)    400800      embedding_5[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, None, 200)    400800      embedding_5[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, None, 200)    0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 700)    0           lstm_7[0][0]                     \n",
      "                                                                 embedding_5[0][0]                \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 100)    70100       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 100)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1010        lambda_9[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 150,873,010\n",
      "Trainable params: 872,710\n",
      "Non-trainable params: 150,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', limit= 500000,binary=True)\n",
    "embeddings = np.zeros((word2vec.syn0.shape[0] + 1, word2vec.syn0.shape[1]), dtype = \"float32\")\n",
    "embeddings[:word2vec.syn0.shape[0]] = word2vec.syn0\n",
    "\n",
    "MAX_TOKENS = word2vec.syn0.shape[0]\n",
    "embedding_dim = word2vec.syn0.shape[1]\n",
    "hidden_dim_1 = 200\n",
    "hidden_dim_2 = 100\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "document = Input(shape = (None, ), dtype = \"int32\")\n",
    "left_context = Input(shape = (None, ), dtype = \"int32\")\n",
    "right_context = Input(shape = (None, ), dtype = \"int32\")\n",
    "\n",
    "embedder = Embedding(MAX_TOKENS + 1, embedding_dim, weights = [embeddings], trainable = False)\n",
    "doc_embedding = embedder(document)\n",
    "l_embedding = embedder(left_context)\n",
    "r_embedding = embedder(right_context)\n",
    "\n",
    "forward = LSTM(hidden_dim_1, return_sequences = True)(l_embedding)\n",
    "backward = LSTM(hidden_dim_1, return_sequences = True, go_backwards = True)(r_embedding)\n",
    "\n",
    "# Keras returns the output sequences in reverse order.\n",
    "backward = Lambda(lambda x: K.reverse(x, axes = 1))(backward)\n",
    "together = concatenate([forward, doc_embedding, backward], axis = 2)\n",
    "\n",
    "semantic = Conv1D(hidden_dim_2, kernel_size = 1, activation = \"tanh\")(together)\n",
    "\n",
    "## define customized maxpooling layer\n",
    "pool_rnn = Lambda(lambda x: K.max(x, axis = 1), output_shape = (hidden_dim_2, ))(semantic)\n",
    "model_output = Dense(NUM_CLASSES, input_dim = hidden_dim_2, activation = \"softmax\")(pool_rnn)\n",
    "\n",
    "model_RCNN = Model(inputs = [document, left_context, right_context], outputs = model_output)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_RCNN.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "model_RCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = TensorBoard()\n",
    "# csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "# filepath= \"weights.best.hdf5\"\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "#                                              monitor='val_categorical_accuracy', \n",
    "#                                              verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# history_rcnn= RCNN_model.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "#                              {'main_output': y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "#                              validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "#                              callbacks=[tb,csv_logger,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some example text.\"\n",
    "text = text.strip().lower().translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "tokens = text.split()\n",
    "# tokens = [voca[token].index if token in word2vec.vocab else MAX_TOKENS for token in tokens]\n",
    "\n",
    "doc_as_array = np.array([tokens])\n",
    "# We shift the document to the right to obtain the left-side contexts.\n",
    "left_context_as_array = np.array([[MAX_TOKENS] + tokens[:-1]])\n",
    "# We shift the document to the left to obtain the right-side contexts.\n",
    "right_context_as_array = np.array([tokens[1:] + [MAX_TOKENS]])\n",
    "\n",
    "target = np.array([NUM_CLASSES * [0]])\n",
    "target[0][3] = 1\n",
    "\n",
    "history33 = model_RCNN.fit([doc_as_array, left_context_as_array, right_context_as_array], target, epochs = 1, verbose = 0)\n",
    "loss = history33.history[\"loss\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
