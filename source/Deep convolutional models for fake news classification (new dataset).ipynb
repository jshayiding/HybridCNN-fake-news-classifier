{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## keras dependencies\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Conv1D, Dense, Input, Lambda, LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import _pickle as cPickle\n",
    "\n",
    "from keras.layers import Concatenate, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, MaxPool1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fake_df=pd.read_csv('gossipcop_fake.csv')\n",
    "legit_df=pd.read_csv('gossipcop_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jshayi2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## remove stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "fake_df['title'] = list(map(' '.join, fake_df['title'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "legit_df['title'] = list(map(' '.join, legit_df['title'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "\n",
    "##\n",
    "fakes = [s.strip().lower() for s in fake_df.title]\n",
    "legits = [s.strip().lower() for s in legit_df.title]\n",
    "\n",
    "## preprocessing utilities\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "## clean text\n",
    "x_text = fakes + legits\n",
    "x_text = [clean_str(sent) for sent in x_text]\n",
    "\n",
    "## \n",
    "# Generate labels\n",
    "# legits_labels = [[0, 1] for _ in legits]\n",
    "# fakes_labels = [[1, 0] for _ in fakes]\n",
    "# y = np.concatenate([legits_labels, fakes_labels], 0)\n",
    "\n",
    "labels = [0 for _ in range(len(fakes))] + [1 for _ in range(len(legits))]\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize for word based embedding\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(x_text)\n",
    "sequences = tokenizer.texts_to_sequences(x_text)\n",
    "vocab_dict=tokenizer.word_index\n",
    "data   = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter\n",
    "MAX_NUM_WORDS   = 15000\n",
    "hidden_size = 150 #Has to be same as EMBEDDING_DIM\n",
    "lstm_size = 100\n",
    "num_steps = 500\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [3,4,5]\n",
    "filter_size = 128\n",
    "embedding_dims=300\n",
    "val_size= 0.2\n",
    "max_features = len(vocab_dict)+1\n",
    "vocab_length = len(vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.randint(234)\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=val_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    }
   ],
   "source": [
    "## loading glovec\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "glove_model = loadGloveModel('glove.6B.300d.txt')\n",
    "\n",
    "def build_glove_embedding_layers():\n",
    "    embed_matrix=np.zeros((max_features, embedding_dims))\n",
    "    for word, indx in tokenizer.word_index.items():\n",
    "        if indx >= max_features:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return embed_matrix\n",
    "\n",
    "embedding_weights=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 500, 300)     5742600     main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 498, 128)     115328      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 500, 128)     115328      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 500, 128)     153728      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 500, 128)     192128      embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 166, 128)     0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling1D) (None, 166, 128)     0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling1D) (None, 166, 128)     0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_55 (MaxPooling1D) (None, 166, 128)     0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 164, 128)     49280       max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_45 (Flatten)            (None, 21248)        0           max_pooling1d_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_46 (Flatten)            (None, 21248)        0           max_pooling1d_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_47 (Flatten)            (None, 21248)        0           max_pooling1d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 54, 128)      0           conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 63744)        0           flatten_45[0][0]                 \n",
      "                                                                 flatten_46[0][0]                 \n",
      "                                                                 flatten_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_48 (Flatten)            (None, 6912)         0           max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 63744)        0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, 6912)         0           flatten_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          8159360     dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          884864      dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 256)          0           dense_17[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 2)            514         concatenate_17[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 15,413,130\n",
      "Trainable params: 9,670,530\n",
      "Non-trainable params: 5,742,600\n",
      "__________________________________________________________________________________________________\n",
      "Train on 17712 samples, validate on 4428 samples\n",
      "Epoch 1/20\n",
      " 3584/17712 [=====>........................] - ETA: 5:27 - loss: 0.5718 - categorical_accuracy: 0.7397"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-3e27134185fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m                            \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                            \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                            callbacks=[tb,csv_logger,checkpoint, es])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embed_sequences = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel, \n",
    "                 padding=\"same\", activation=\"relu\", strides=1)(embed_sequences)\n",
    "    x_1 = MaxPool1D(3)(x_1)\n",
    "    x_flat = Flatten()(x_1)\n",
    "    x_drop = Dropout(0.85)(x_flat)\n",
    "    kernel_arr.append(x_flat)\n",
    "\n",
    "conv_ins = keras.layers.concatenate(kernel_arr)\n",
    "conv_ins = Dropout(0.85)(conv_ins)\n",
    "conv_ins = Dense(128, activation='relu')(conv_ins)\n",
    "\n",
    "## add multi convolutional layer\n",
    "l_conv1 = Conv1D(128,3,activation=\"relu\", padding=\"valid\", strides=1)(embed_sequences)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128, 3, activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPooling1D(3)(l_conv2)\n",
    "# l_conv3 = Conv1D(128,3,activation=\"relu\")(l_pool2)\n",
    "# l_pool3 = MaxPool1D(3)(l_conv3)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "conv_in1 = Dropout(0.85)(l_flat)\n",
    "conv_in1 = Dense(128, activation='relu')(conv_in1)\n",
    "\n",
    "## do merge\n",
    "conv_merged= keras.layers.concatenate([conv_ins, conv_in1])\n",
    "\n",
    "# #Meta input\n",
    "# meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "# x_drop = Dropout(0.9)(meta_input)\n",
    "# x_meta = Dense(100, activation='relu')(x_drop)\n",
    "# x = keras.layers.concatenate([conv_merged, x_meta])\n",
    "\n",
    "\n",
    "main_output = Dense(2, activation='softmax', name='main_output')(conv_merged)\n",
    "model_hybird = Model(inputs=[statement_input], outputs=[main_output])\n",
    "\n",
    "#************************************************************************#\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "model_hybird.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_hybird.summary()\n",
    "\n",
    "##\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history22= model_hybird.fit(X_train,\n",
    "                           y_train,epochs=20, batch_size=128,\n",
    "                           validation_data=(X_val,y_val),\n",
    "                           callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# this part is used to create metadata for given dataset\\nurls = fake_df.news_url.dropna()\\nmake_me = []\\nfor url in urls:\\n    lst = url.split(\"/\")\\n    lst[-2] = lst[-2][:-12]\\n    make_me.append([x for x in lst if not x.isdigit() and not x == \"\"])\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# this part is used to create metadata for given dataset\n",
    "urls = fake_df.news_url.dropna()\n",
    "make_me = []\n",
    "for url in urls:\n",
    "    lst = url.split(\"/\")\n",
    "    lst[-2] = lst[-2][:-12]\n",
    "    make_me.append([x for x in lst if not x.isdigit() and not x == \"\"])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
