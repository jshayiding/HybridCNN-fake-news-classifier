{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.utils\n",
    "import nlp_util\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Conv1D, Dense, Input, Lambda, LSTM\n",
    "from keras.layers import Bidirectional, Lambda, TimeDistributed\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import _pickle as cPickle\n",
    "\n",
    "from keras.layers import Concatenate, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, MaxPool1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, CSVLogger, EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dataset\n",
    "train_file=pd.read_csv('train.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "test_file=pd.read_csv('test.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "va_file=pd.read_csv('valid.tsv', sep='\\t', header=None, encoding='utf-8')\n",
    "\n",
    "column_names = ['Id', 'Label','Statement','Subject','Speaker','Speaker Job','State Info','Party','BT','FC','HT','MT','PF','Context']\n",
    "train_file.columns, test_file.columns, va_file.columns=column_names, column_names, column_names\n",
    "\n",
    "train_data = train_file[train_file.columns[~train_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]\n",
    "test_data = test_file[test_file.columns[~test_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]\n",
    "val_data = va_file[va_file.columns[~va_file.columns.isin(['Id','BT','FC','HT','MT','PF'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_labels_dict = {'false':0, 'true':1,'pants-fire':2,'barely-true':3,'half-true':4,'mostly-true':5}\n",
    "binary_labels = {'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}\n",
    "\n",
    "\n",
    "def one_hot_label(label):\n",
    "    return to_categorical(multi_labels_dict[x], num_classes=6)\n",
    "\n",
    "train_data['multi_label']=train_data['Label'].apply(lambda x: multi_labels_dict[x])\n",
    "test_data['multi_label']=test_data['Label'].apply(lambda x: multi_labels_dict[x])\n",
    "val_data['multi_label']=val_data['Label'].apply(lambda x: multi_labels_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers= ['barack-obama', 'donald-trump', 'hillary-clinton', 'mitt-romney', \n",
    "                'scott-walker', 'john-mccain', 'rick-perry', 'chain-email', \n",
    "                'marco-rubio', 'rick-scott', 'ted-cruz', 'bernie-s', 'chris-christie', \n",
    "                'facebook-posts', 'charlie-crist', 'newt-gingrich', 'jeb-bush', \n",
    "                'joe-biden', 'blog-posting','paul-ryan']\n",
    "\n",
    "speaker_dict={}\n",
    "for cnt, speaker in enumerate(speakers):\n",
    "    speaker_dict[speaker]=cnt\n",
    "print(speaker_dict)\n",
    "\n",
    "def speaker_projection(speaker):\n",
    "    if isinstance(speaker, str):\n",
    "        speaker=speaker.lower()\n",
    "        matched=[s for s in speakers if s in speaker]\n",
    "        if len(matched)>0:\n",
    "            return speaker_dict[matched[0]]\n",
    "        else:\n",
    "            return len(speakers)\n",
    "        \n",
    "##\n",
    "train_data['speaker_id']=train_data['Speaker'].apply(speaker_projection)\n",
    "test_data['speaker_id']=test_data['Speaker'].apply(speaker_projection)\n",
    "val_data['speaker_id']=val_data['Speaker'].apply(speaker_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map job\n",
    "job_list = ['president', 'u.s. senator', 'governor', 'president-elect', 'presidential candidate', \n",
    "                'u.s. representative', 'state senator', 'attorney', 'state representative', 'congress', 'others']\n",
    "\n",
    "\n",
    "job_dict = {'president':0, 'u.s. senator':1, 'governor':2, 'president-elect':3, 'presidential candidate':4, \n",
    "            'u.s. representative':5, 'state senator':6, 'attorney':7, 'state representative':8, 'congress':9, 'others':10}\n",
    "\n",
    "## Map job\n",
    "\n",
    "def job_projection(job):\n",
    "    if isinstance(job, str):\n",
    "        job=job.lower()\n",
    "        matched_job=[j for j in job_list if j in job]\n",
    "        if len(matched_job)>0:\n",
    "            return job_dict[matched_job[0]]\n",
    "        else:\n",
    "            return 10\n",
    "    else:\n",
    "        return 10\n",
    "\n",
    "## job projection output\n",
    "\n",
    "train_data['job_id']=train_data['Speaker Job'].apply(job_projection)\n",
    "test_data['job_id']=test_data['Speaker Job'].apply(job_projection)\n",
    "val_data['job_id']=val_data['Speaker Job'].apply(job_projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Map political parties\n",
    "party_dict={'republican':0, 'democrat':1, 'none':2, 'organization':3, 'newsmaker':4, 'rest':5}\n",
    "\n",
    "def map_political_party(party):\n",
    "    if party in party_dict:\n",
    "        return party_dict[party]\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "##\n",
    "train_data['party_id']=train_data['Party'].apply(map_political_party)\n",
    "test_data['party_id']=test_data['Party'].apply(map_political_party)\n",
    "val_data['party_id']=val_data['Party'].apply(map_political_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map states\n",
    "all_states = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado',\n",
    "              'Connecticut','Delaware','Florida','Georgia','Hawaii','Idaho', \n",
    "              'Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana',\n",
    "              'Maine' 'Maryland','Massachusetts','Michigan','Minnesota',\n",
    "              'Mississippi', 'Missouri','Montana','Nebraska','Nevada',\n",
    "              'New Hampshire','New Jersey','New Mexico','New York',\n",
    "              'North Carolina','North Dakota','Ohio',    \n",
    "              'Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "              'South  Carolina','South Dakota','Tennessee','Texas','Utah',\n",
    "              'Vermont','Virginia','Washington','West Virginia',\n",
    "              'Wisconsin','Wyoming']\n",
    "\n",
    "\n",
    "states_dict = {'wyoming': 48, 'colorado': 5, 'washington': 45, 'hawaii': 10, \n",
    "               'tennessee': 40, 'wisconsin': 47, 'nevada': 26, 'north dakota': 32, \n",
    "               'mississippi': 22, 'south dakota': 39, 'new jersey': 28, 'oklahoma': 34, \n",
    "               'delaware': 7, 'minnesota': 21, 'north carolina': 31, 'illinois': 12, \n",
    "               'new york': 30, 'arkansas': 3, 'west virginia': 46, 'indiana': 13, \n",
    "               'louisiana': 17, 'idaho': 11, 'south  carolina': 38, 'arizona': 2, \n",
    "               'iowa': 14, 'mainemaryland': 18, 'michigan': 20, 'kansas': 15, \n",
    "               'utah': 42, 'virginia': 44, 'oregon': 35, 'connecticut': 6, 'montana': 24, \n",
    "               'california': 4, 'massachusetts': 19, 'rhode island': 37, 'vermont': 43, \n",
    "               'georgia': 9, 'pennsylvania': 36, 'florida': 8, 'alaska': 1, 'kentucky': 16,\n",
    "               'nebraska': 25, 'new hampshire': 27, 'texas': 41, 'missouri': 23, 'ohio': 33,\n",
    "               'alabama': 0, 'new mexico': 29, 'rest':50}\n",
    "\n",
    "\n",
    "def state_projection(state):\n",
    "    if isinstance(state, str):\n",
    "        state=state.lower()\n",
    "        if state in states_dict:\n",
    "            return states_dict[state]\n",
    "        else:\n",
    "            if 'washington' in state:\n",
    "                return states_dict['washington']\n",
    "            else:\n",
    "                return 50\n",
    "    else:\n",
    "        return 50\n",
    "    \n",
    "## state mapping output:\n",
    "train_data['state_id']=train_data['State Info'].apply(state_projection)\n",
    "test_data['state_id']=test_data['State Info'].apply(state_projection)\n",
    "val_data['state_id']=val_data['State Info'].apply(state_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## map subject\n",
    "subject_list = ['health','tax','immigration','election','education',\n",
    "    'candidates-biography','economy','gun','jobs','federal-budget','energy','abortion','foreign-policy']\n",
    "\n",
    "subject_dict = {'health':0,'tax':1,'immigration':2,'election':3,'education':4,\n",
    "                'candidates-biography':5,'economy':6,'gun':7,'jobs':8,'federal-budget':9,\n",
    "                'energy':10,'abortion':11,'foreign-policy':12, 'others':13}\n",
    "\n",
    "## mapping subject\n",
    "def subject_projection(subject):\n",
    "    if isinstance(subject, str):\n",
    "        subject=subject.lower()\n",
    "        matched_subject=[subj for subj in subject_list if subj in subject]\n",
    "        \n",
    "        if len(matched_subject)>0:\n",
    "            return subject_dict[matched_subject[0]]\n",
    "        else:\n",
    "            return 13\n",
    "    else:\n",
    "        return 13\n",
    "    \n",
    "##\n",
    "train_data['subject_id']=train_data['Subject'].apply(subject_projection)\n",
    "test_data['subject_id']=test_data['Subject'].apply(subject_projection)\n",
    "val_data['subject_id']=val_data['Subject'].apply(subject_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context mapping\n",
    "Context_list = ['news release','interview','tv','radio',\n",
    "                'campaign','news conference','press conference','press release',\n",
    "                'tweet','facebook','email']\n",
    "\n",
    "Context_dict = {'news release':0,'interview':1,'tv':2,'radio':3,\n",
    "                'campaign':4,'news conference':5,'press conference':6,'press release':7,\n",
    "                'tweet':8,'facebook':9,'email':10, 'others':11}\n",
    "\n",
    "def Context_projection(context):\n",
    "    if isinstance(context, str):\n",
    "        context=context.lower()\n",
    "        matched_context=[cntx for cntx in Context_list if cntx in context]\n",
    "        if len(matched_context)>0:\n",
    "            return Context_dict[matched_context[0]]\n",
    "        else:\n",
    "            return 11\n",
    "    else:\n",
    "        return 11\n",
    "    \n",
    "## context projection output\n",
    "train_data['context_id']=train_data['Context'].apply(Context_projection)\n",
    "test_data['context_id']=test_data['Context'].apply(Context_projection)\n",
    "val_data['context_id']=val_data['Context'].apply(Context_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenize fake news statement and build vocabulary\n",
    "vocab_dict={}\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_data['Statement'])\n",
    "vocab_dict=tokenizer.word_index\n",
    "cPickle.dump(tokenizer.word_index, open(\"vocab.p\",\"wb\"))\n",
    "print(\"vocab dictionary is created\")\n",
    "print(\"saved vocan dictionary to pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab_dict.keys())\n",
    "hidden_size = 150 #Has to be same as EMBEDDING_DIM\n",
    "lstm_size = 100\n",
    "num_steps = 32\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "#Hyperparams for CNN\n",
    "kernel_sizes = [3,4,5]\n",
    "filter_size = 128\n",
    "#Meta data related hyper params\n",
    "num_party = 6\n",
    "num_state = 51\n",
    "num_context = 12\n",
    "num_job = 11\n",
    "num_sub = 14\n",
    "num_speaker = 21\n",
    "embedding_dims=300\n",
    "max_features = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab_dict.keys())\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create embedding layer\n",
    "num_words=len(vocab_dict)+1\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    embeddings_index = {}\n",
    "    f = open(gloveFile, encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "glove_model = loadGloveModel('glove.6B.300d.txt')\n",
    "\n",
    "def build_glove_embedding_layers():\n",
    "    embed_matrix=np.zeros((max_features, embedding_dims))\n",
    "    for word, indx in tokenizer.word_index.items():\n",
    "        if indx >= max_features:\n",
    "            continue\n",
    "        if word in glove_model:\n",
    "            embed_vec=glove_model[word]\n",
    "            if embed_vec is not None:\n",
    "                embed_matrix[indx]=embed_vec\n",
    "    return embed_matrix\n",
    "\n",
    "embedding_weights=build_glove_embedding_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data preprocessing\n",
    "def preprocessing_txt_keras(statement):\n",
    "    txt=text_to_word_sequence(statement)\n",
    "    val=[0]*32\n",
    "    val=[vocab_dict[t] for t in txt if t in vocab_dict]   ##replace unknown words with zero index\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training instances list\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "### remove stopwords fitst\n",
    "train_data['Statement'] = list(map(' '.join, train_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "test_data['Statement'] = list(map(' '.join, test_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))\n",
    "val_data['Statement'] = list(map(' '.join, val_data['Statement'].apply(lambda x: [item for item in x.lower().split() if item not in stop])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['word_id']=train_data['Statement'].apply(preprocessing_txt_keras)\n",
    "test_data['word_id']=test_data['Statement'].apply(preprocessing_txt_keras)\n",
    "val_data['word_id']=val_data['Statement'].apply(preprocessing_txt_keras)\n",
    "\n",
    "x_train=train_data['word_id']\n",
    "x_test=test_data['word_id']\n",
    "x_val=val_data['word_id']\n",
    "\n",
    "y_train=train_data['multi_label']\n",
    "y_val=val_data['multi_label']\n",
    "\n",
    "##\n",
    "x_train=sequence.pad_sequences(x_train, maxlen=num_steps, padding='post', truncating='post')\n",
    "y_train=to_categorical(y_train, num_classes=6)\n",
    "x_val=sequence.pad_sequences(x_val, maxlen=num_steps, padding='post', truncating='post')\n",
    "y_val=to_categorical(y_val, num_classes=6)\n",
    "x_test=sequence.pad_sequences(x_test, maxlen=num_steps, padding='post', truncating='post')\n",
    "\n",
    "## meta data preparation\n",
    "tr_party=to_categorical(train_data['party_id'], num_classes=num_party)\n",
    "tr_state=to_categorical(train_data['state_id'], num_classes=num_state)\n",
    "tr_cont=to_categorical(train_data['context_id'], num_classes=num_context)\n",
    "tr_job=to_categorical(train_data['job_id'], num_classes=num_job)\n",
    "tr_subj=to_categorical(train_data['subject_id'], num_classes=num_sub)\n",
    "# tr_speaker=to_categorical(train_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "# x_train_metadata=np.hstack(tr_party, tr_state, tr_job, tr_subj, tr_speaker, tr_cont)\n",
    "x_train_metadata=np.hstack((tr_party, tr_state, tr_cont,tr_job, tr_subj))\n",
    "\n",
    "\n",
    "# #********************************************************************************#\n",
    "val_party=to_categorical(val_data['party_id'], num_classes=num_party)\n",
    "val_state=to_categorical(val_data['state_id'], num_classes=num_state)\n",
    "val_cont=to_categorical(val_data['context_id'], num_classes=num_context)\n",
    "val_job=to_categorical(val_data['job_id'], num_classes=num_job)\n",
    "val_subj=to_categorical(val_data['subject_id'], num_classes=num_sub)\n",
    "# val_speaker=to_categorical(val_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "# x_val_metadata=np.hstack(val_party, val_state, val_job, val_subj, val_speaker, val_cont)\n",
    "x_val_metadata=np.hstack((val_party, val_state, val_cont,val_job, val_subj, ))\n",
    "\n",
    "# #********************************************************************************#\n",
    "te_party=to_categorical(test_data['party_id'], num_classes=num_party)\n",
    "te_state=to_categorical(test_data['state_id'], num_classes=num_state)\n",
    "te_cont=to_categorical(test_data['context_id'], num_classes=num_context)\n",
    "te_job=to_categorical(test_data['job_id'], num_classes=num_job)\n",
    "te_subj=to_categorical(test_data['subject_id'], num_classes=num_sub)\n",
    "# te_speaker=to_categorical(test_data['speaker_id'], num_classes=num_speaker)\n",
    "\n",
    "# ## put all metadata of train data together in one stack\n",
    "x_test_metadata=np.hstack((te_party, te_state, te_cont,te_job, te_subj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  state of art CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to initialize model weight\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embed_sequences = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel, \n",
    "                 padding=\"valid\", activation=\"relu\", strides=1)(embed_sequences)\n",
    "    x_1 = MaxPool1D(3)(x_1)\n",
    "    x_flat = Flatten()(x_1)\n",
    "    x_drop = Dropout(0.9)(x_flat)\n",
    "    kernel_arr.append(x_drop)\n",
    "\n",
    "conv_in = keras.layers.concatenate(kernel_arr)\n",
    "# conv_in = Dropout(0.85)(conv_in)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_drop = Dropout(0.7)(meta_input)\n",
    "x_meta = Dense(64, activation='relu')(x_drop)\n",
    "x = keras.layers.concatenate([conv_in, x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "#************************************************************************#\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('state_art_CNN.csv', append=True, separator=';')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"state_art_CNN.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history11 = model.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                   {'main_output': y_train},epochs=20, batch_size=100,\n",
    "                   validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                  callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_curve=plt.figure()\n",
    "plt.plot(history11.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history11.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : CNN',fontsize=12)\n",
    "# accu_curve.savefig('accuracy_cnn_improved_v1.6.best.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history11.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history11.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :CNN',fontsize=12)\n",
    "# loss_curve.savefig('loss_cnn_improved_v1.6.best.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Multi-convolutional CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do small modification on above CNN model, in this notebook I introduce multi convolutional CNN model for fake news classification. let's see how it works and see where is significancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_input = Input(shape=(num_steps,), dtype='int64', name='main_input')\n",
    "embed_sequence = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "\n",
    "## add multi convolutional layer\n",
    "l_conv1 = Conv1D(128,4,activation=\"relu\", padding=\"same\", strides=1)(embed_sequence)\n",
    "l_pool1 = MaxPooling1D(4)(l_conv1)\n",
    "l_conv2 = Conv1D(128, 4, activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPooling1D(4)(l_conv2)\n",
    "# l_conv3 = Conv1D(128,3,activation=\"relu\")(l_pool2)\n",
    "# l_pool3 = MaxPooling1D()(l_conv3)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "conv_in = Dropout(0.9)(l_flat)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_drop = Dropout(0.7)(meta_input)\n",
    "x_meta = Dense(64, activation='relu')(x_drop)\n",
    "x = keras.layers.concatenate([conv_in, x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model_multi = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "#************************************************************************#\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_multi.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_multi.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('multi_conv_CNN.csv', append=True, separator=';')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"multi_conv_CNN.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history12= model_multi.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                           {'main_output': y_train},epochs=18, batch_size=64,\n",
    "                           validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                           callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to see performance difference\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(history12.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history12.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : CNN_multi',fontsize=12)\n",
    "# accu_curve.savefig('accuracy_multi_conv_cnn_v.1.6.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history12.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history12.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :CNN Multi',fontsize=12)\n",
    "# loss_curve.savefig('loss_multi_conv_cnnv.1.6.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Hybrid CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embed_sequences = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel, \n",
    "                 padding=\"same\", activation=\"relu\", strides=1)(embed_sequences)\n",
    "    x_1 = MaxPool1D(3)(x_1)\n",
    "    x_flat = Flatten()(x_1)\n",
    "    x_drop = Dropout(0.85)(x_flat)\n",
    "    kernel_arr.append(x_drop)\n",
    "\n",
    "conv_ins = keras.layers.concatenate(kernel_arr)\n",
    "# conv_ins = Dropout(0.85)(conv_ins)\n",
    "conv_ins = Dense(128, activation='relu')(conv_ins)\n",
    "\n",
    "## add multi convolutional layer\n",
    "l_conv1 = Conv1D(128,3,activation=\"relu\", padding=\"valid\", strides=1)(embed_sequences)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128, 3, activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPooling1D(3)(l_conv2)\n",
    "# l_conv3 = Conv1D(128,3,activation=\"relu\")(l_pool2)\n",
    "# l_pool3 = MaxPool1D(3)(l_conv3)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "conv_in1 = Dropout(0.85)(l_flat)\n",
    "conv_in1 = Dense(128, activation='relu')(conv_in1)\n",
    "\n",
    "## do merge\n",
    "conv_merged= keras.layers.concatenate([conv_ins, conv_in1])\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_drop = Dropout(0.85)(meta_input)\n",
    "x_meta = Dense(64, activation='relu')(x_drop)\n",
    "x = keras.layers.concatenate([conv_merged, x_meta])\n",
    "\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model_hybird = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "#************************************************************************#\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "model_hybird.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_hybird.summary()\n",
    "\n",
    "##\n",
    "tb = TensorBoard()\n",
    "csv_logger =  keras.callbacks.CSVLogger('hybird_CNN.csv', append=True, separator=';')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "filepath= \"hybird_CNN.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history13= model_hybird.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                           {'main_output': y_train},epochs=12, batch_size=100,\n",
    "                           validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                           callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to see performance difference\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(history13.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history13.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : CNN_hubrid',fontsize=12)\n",
    "# accu_curve.savefig('accuracy_CNN_hubrid.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history13.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history13.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :CNN_hybrid',fontsize=12)\n",
    "# loss_curve.savefig('loss_CNN_hybrid.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  C-LSTM model for fake news classification\n",
    "\n",
    "To better understand performance gap between different convolutional models, here I want to see Convolutional -LSTM model for fake news classification. Note that I didn't run multiple convolutional filers for fake news texts because it raise strange error, so I'll proceed somehow simple but powerful model for fake news classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C-LSTM model implementation (trail version)\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "statement_input = Input(shape=(num_steps,), dtype='int64', name='main_input')\n",
    "# embed_layer = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "embed_layer = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "l_lstm = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(embed_layer)\n",
    "# lstm_pool = GlobalMaxPool1D()(l_lstm)\n",
    "\n",
    "## let's add convolutional layer on it\n",
    "l_conv1 = Conv1D(128, 3, activation=\"relu\")(l_lstm)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128,3,activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPool1D(3)(l_conv2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_drop = Dropout(0.8)(l_flat)\n",
    "conv_in = Dense(hidden_size, activation=\"relu\")(l_drop)\n",
    "\n",
    "### metadata\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_meta_hidden = Dense(64, activation='relu')(meta_input)\n",
    "x_meta_reg = Dropout(0.6)(x_meta_hidden)\n",
    "\n",
    "conv_final = keras.layers.concatenate([conv_in, x_meta_reg])\n",
    "model_output = Dense(6, activation='softmax', name='main_output')(conv_final)\n",
    "model_CLSTM = Model(inputs=[statement_input, meta_input], outputs=[model_output])\n",
    "\n",
    "## compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_CLSTM.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_CLSTM.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('CLSTM.csv', append=True, separator=';')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"CLSTM.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history_clstm = model_CLSTM.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                                {'main_output': y_train},epochs=num_epochs, batch_size=batch_size,\n",
    "                                validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                                callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(history_clstm.history).to_csv(\"history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experimental modification on c-lstm model for fake news classification\n",
    "\n",
    "\"\"\"\n",
    "## let's add lstm\n",
    "l_lstm = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(embed_layer)\n",
    "lstm_pool = GlobalMaxPool1D()(l_lstm)\n",
    "lstm_drop = Dropout(0.6)(lstm_pool)\n",
    "lstm_dense = Dense(hidden_size, activation=\"relu\")(lstm_drop)\n",
    "\n",
    "## let's add convolutional layer\n",
    "l_conv1 = Conv1D(128, 3, activation=\"relu\")(embed_layer)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128,3,activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPool1D(3)(l_conv2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_drop = Dropout(0.6)(l_flat)\n",
    "conv_in = Dense(hidden_size, activation=\"relu\")(l_drop)\n",
    "\n",
    "## merge lstm and convs\n",
    "conv_ins = keras.layers.concatenate([conv_in, lstm_dense])\n",
    "\n",
    "### metadata\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_meta_hidden = Dense(64, activation='relu')(meta_input)\n",
    "x_meta_reg = Dropout(0.6)(x_meta_hidden)\n",
    "\n",
    "conv_final = keras.layers.concatenate([conv_ins, x_meta_reg])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize c-lstm model output\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(history_clstm.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history_clstm.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : C-LSTM',fontsize=12)\n",
    "# accu_curve.savefig('accuracy_clstm_improved_v1.6.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history_clstm.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history_clstm.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :C-LSTM',fontsize=12)\n",
    "# loss_curve.savefig('loss_clstm_improved_v1.6.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified version of C-LSTM model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++##\n",
    "kernel_arr = []\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embedded_sequences = Embedding(vocab_length+1,embedding_dims,weights=[embedding_weights],input_length=num_steps,trainable=False)(statement_input) #Preloaded glove embeddings\n",
    "# x = Embedding(output_dim=hidden_size, input_dim=vocab_length+1, input_length=num_steps)(statement_input) #Train embeddings from scratch\n",
    "x_lstm = Bidirectional(LSTM(128, return_sequences=True, dropout=0.4, recurrent_dropout=0.5))(embedded_sequences)\n",
    "\n",
    "for kernel in kernel_sizes:\n",
    "    x_1 = Conv1D(filters=filter_size,kernel_size=kernel, \n",
    "                 padding=\"valid\", activation=\"relu\", strides=1)(x_lstm)\n",
    "    x_pool = MaxPool1D(4)(x_1)\n",
    "    #x_pool = GlobalMaxPool1D()(x_1)\n",
    "    x_flat = Flatten()(x_pool)\n",
    "    x_drop = Dropout(0.85)(x_flat)\n",
    "    kernel_arr.append(x_drop)\n",
    "\n",
    "conv_in = keras.layers.concatenate(kernel_arr)\n",
    "# conv_in = Dropout(0.7)(conv_in)\n",
    "conv_in = Dense(128, activation='relu')(conv_in)\n",
    "\n",
    "\"\"\"\n",
    "## use bidirectional lstm\n",
    "x_lstm = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x_1)\n",
    "lstm_pool = GlobalMaxPool1D()(x_lstm)\n",
    "lstm_dense = Dense(100, activation=\"relu\")(lstm_pool)\n",
    "\n",
    "## first concatenate\n",
    "conv_ins= keras.layers.concatenate([conv_in, lstm_dense])\n",
    "\"\"\"\n",
    "\n",
    "#Meta input\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_drop = Dropout(0.8)(meta_input)\n",
    "x_meta = Dense(64, activation='relu')(x_drop)\n",
    "x = keras.layers.concatenate([conv_in,x_meta])\n",
    "\n",
    "main_output = Dense(6, activation='softmax', name='main_output')(x)\n",
    "model_clstm_ = Model(inputs=[statement_input, meta_input], outputs=[main_output])\n",
    "\n",
    "\n",
    "## compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_clstm_.compile(optimizer=sgd,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model_clstm_.summary()\n",
    "\n",
    "##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++##\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('Hybird_CLSTM.csv', append=True, separator=';')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "filepath= \"hybird_CLSTM.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history_clstm_ = model_clstm_.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                                {'main_output': y_train},epochs=num_epochs, batch_size=64,\n",
    "                                validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                                callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(history_clstm_.history).to_csv(\"history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize c-lstm model output\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(history_clstm_.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history_clstm_.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : C-LSTM',fontsize=12)\n",
    "# accu_curve.savefig('accuracy_clstm_improved_v1.6.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history_clstm_.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history_clstm_.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves :C-LSTM',fontsize=12)\n",
    "# loss_curve.savefig('loss_clstm_improved_v1.6.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from math import sqrt\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Concatenate, Embedding, Flatten\n",
    "from keras.layers import AlphaDropout\n",
    "from keras.layers import ThresholdedReLU\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### character level classification (simple version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers=[[256, 7, 3],\n",
    "             [256, 7, 3],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, -1],\n",
    "             [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers=[1024, 1024]\n",
    "dropout_p=0.5\n",
    "threshold=1e-06\n",
    "input_size=70\n",
    "alphabet='abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "\n",
    "batch_size=128\n",
    "checkpoint_every=70\n",
    "epochs=5000\n",
    "# evaluate_every=100\n",
    "batch_size=128\n",
    "checkpoint_every=100\n",
    "epochs=5000\n",
    "# evaluate_every=100\n",
    "embedding_size=128\n",
    "alphabet_size=69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## define model input, create embedding layer\n",
    "sent_inputs = Input(shape=(input_size,), name='sent_input', dtype='int64')  # shape=(?, 1014)\n",
    "\n",
    "conv = Embedding(alphabet_size+1, embedding_size, input_length=input_size, trainable=False)(sent_inputs)\n",
    "# Conv \n",
    "conv_lay=[]\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x_conv = Conv1D(filter_num, filter_size, padding=\"same\")(conv) \n",
    "    x_pool  = GlobalMaxPooling1D()(x_conv)\n",
    "    x_drop = Dropout(0.6)(x_pool)\n",
    "    conv_lay.append(x_drop)\n",
    "#     if pooling_size != -1:\n",
    "#         conv = MaxPooling1D(pool_size=pooling_size)(conv) # Final shape=(None, 34, 256)\n",
    "\n",
    "# x = Flatten()(conv_lay) # (None, 8704)\n",
    "x=concatenate(conv_lay)\n",
    "\n",
    "# Fully connected layers \n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x) # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "    \n",
    "model_output = Dense(6, activation='softmax')(x)\n",
    "# Build model\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.2)\n",
    "\n",
    "model_char = Model(inputs=sent_inputs, outputs=model_output)\n",
    "model_char.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "model_char.summary()\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('training.log')\n",
    "es=EarlyStopping(monitor=\"val_loss\", mode='min', verbose=1, patience=3 )\n",
    "filepath= \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "# history_char= model_char.fit(x_train,y_train,epochs=num_epochs, batch_size=batch_size,\n",
    "#                                validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_char.fit(x_train,y_train,epochs=10, batch_size=64, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Recurrent CNN model for fake news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_1 = 128\n",
    "hidden_dim_2 = 128\n",
    "\n",
    "kernel_arr = []\n",
    "\n",
    "statement_input = Input(shape=(num_steps,), dtype='int32', name='main_input')\n",
    "embed_sequences = Embedding(vocab_length+1,embedding_dims,\n",
    "                            weights=[embedding_weights],input_length=num_steps,\n",
    "                            trainable=False)(statement_input)\n",
    "\n",
    "L1 = Bidirectional(LSTM(hidden_dim_1, return_sequences=True))(embed_sequences)\n",
    "L2 = TimeDistributed(Dense(hidden_dim_2, activation = \"tanh\"))(L1)\n",
    "\n",
    "l_conv1 = Conv1D(128, 3, activation=\"relu\")(L2)\n",
    "l_pool1 = MaxPooling1D(3)(l_conv1)\n",
    "l_conv2 = Conv1D(128,3,activation=\"relu\")(l_pool1)\n",
    "l_pool2 = MaxPool1D(3)(l_conv2)\n",
    "pool_rnn = Lambda(lambda x: K.max(x, axis = 1), \n",
    "                  output_shape = (hidden_dim_2, ))(l_pool2)\n",
    "# l_flat = Flatten()(pool_rnn)\n",
    "l_drop = Dropout(0.8)(pool_rnn)\n",
    "conv_in = Dense(hidden_size, activation=\"relu\")(l_drop)\n",
    "\n",
    "### metadata\n",
    "meta_input = Input(shape=(x_train_metadata.shape[1],), name='aux_input')\n",
    "x_meta_hidden = Dense(64, activation='relu')(meta_input)\n",
    "x_meta_reg = Dropout(0.6)(x_meta_hidden)\n",
    "\n",
    "conv_final = keras.layers.concatenate([conv_in, x_meta_reg])\n",
    "model_output = Dense(6, activation='softmax', name='main_output')(conv_final)\n",
    "model_RCNN = Model(inputs=[statement_input, meta_input], outputs=[model_output])\n",
    "\n",
    "## compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_RCNN.compile(optimizer=sgd,\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['categorical_accuracy'])\n",
    "\n",
    "model_RCNN.summary()\n",
    "\n",
    "tb = TensorBoard()\n",
    "csv_logger = keras.callbacks.CSVLogger('RCNN.csv', append=True, separator=';')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "filepath= \"RCNN.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_categorical_accuracy', \n",
    "                                             verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history_RCNN = model_RCNN.fit({'main_input': x_train, 'aux_input': x_train_metadata},\n",
    "                               {'main_output': y_train},epochs=20, batch_size=batch_size,\n",
    "                               validation_data=({'main_input': x_val, 'aux_input': x_val_metadata},{'main_output': y_val}),\n",
    "                               callbacks=[tb,csv_logger,checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize RCNN\n",
    "\n",
    "### visualize c-lstm model output\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(history_RCNN.history['categorical_accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history_RCNN.history['val_categorical_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Accuracy Curves : RCNN',fontsize=12)\n",
    "# accu_curve.savefig('accuracy_clstm_improved_v1.4.4.png')\n",
    "plt.show()\n",
    "##^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^##\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(history_RCNN.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history_RCNN.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "plt.xlabel('Epochs ',fontsize=12)\n",
    "plt.ylabel('Loss',fontsize=12)\n",
    "plt.title('Loss Curves : RCNN',fontsize=12)\n",
    "# loss_curve.savefig('loss_clstm_improved_v1.4.4.png')\n",
    "loss_curve.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
