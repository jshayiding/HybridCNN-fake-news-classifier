{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    }
   ],
   "source": [
    "# pos tagging for fake news statement\n",
    "## use pos-tagging to build features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# import utility libraries\n",
    "import util\n",
    "import preprocessing\n",
    "import importlib\n",
    "import string\n",
    "\n",
    "# libraries for model testing and selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP_Task ready to use.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'preprocessing' from 'D:\\\\UIC\\\\Fall 2018\\\\Statistical NLP\\\\Project\\\\jurat-aldo-project\\\\cs-521-project.git\\\\source\\\\CS-521-PROJECT\\\\preprocessing.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "tr_file, va_file, te_file = util.load_files()\n",
    "tr_dict = util.tsv_to_dict(tsv_file=tr_file)\n",
    "va_dict = util.tsv_to_dict(tsv_file=va_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data without punctuation\n",
    "tr_data_no_punctuation = preprocessing.clean_text(sentences=tr_dict['statement'], remove_punctuation=True)\n",
    "va_data_no_punctuation = preprocessing.clean_text(sentences=va_dict['statement'], remove_punctuation=True)\n",
    "\n",
    "## Data without punctuation and uppercases\n",
    "tr_data_no_punct_upper = preprocessing.clean_text(sentences=tr_dict['statement'], remove_punctuation=True, lower_case= True)\n",
    "va_data_no_punct_upper = preprocessing.clean_text(sentences=va_dict['statement'], remove_punctuation=True, lower_case= True)\n",
    "\n",
    "## Data without punctuation, uppercases and stopwords\n",
    "tr_data_no_punct_upper_stopw = preprocessing.clean_text(sentences=tr_dict['statement'], remove_punctuation=True, lower_case=True, stop_words=True)\n",
    "va_data_no_punct_upper_stopw = preprocessing.clean_text(sentences=va_dict['statement'], remove_punctuation=True, lower_case=True, stop_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting POS tags grouped by unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n",
      "Finished\n",
      "Extracting POS Tags\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-09cd7a9893b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# POS withoutn punctuation and with lower case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0munigram_pos_no_p_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigrams_pos_no_p_u\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram_pos_no_p_u\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_POS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_data_no_punct_upper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0munigram_pos_no_p_u_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigrams_pos_no_p_u_va\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrigram_pos_no_p_u_va\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_POS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mva_data_no_punct_upper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\UIC\\Fall 2018\\Statistical NLP\\Project\\jurat-aldo-project\\cs-521-project.git\\source\\CS-521-PROJECT\\preprocessing.py\u001b[0m in \u001b[0;36mextract_POS\u001b[1;34m(statements)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_POS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Extracting POS Tags'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mpos_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOS_tagging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatements\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_word_tag_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mbigrams_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOS_groupping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mtrigrams_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorenlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOS_groupping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\UIC\\Fall 2018\\Statistical NLP\\Project\\jurat-aldo-project\\cs-521-project.git\\source\\CS-521-PROJECT\\nlp_util.py\u001b[0m in \u001b[0;36mPOS_tagging\u001b[1;34m(self, statements, return_word_tag_pairs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \t\t\tannotations = self.core_nlp.annotate(statement, properties={\n\u001b[0;32m     21\u001b[0m                           \u001b[1;34m'annotators'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'tokenize,pos'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                           \u001b[1;34m'outputFormat'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \t\t\t  })\n\u001b[0;32m     24\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pycorenlp\\corenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[1;34m(self, text, properties)\u001b[0m\n\u001b[0;32m     27\u001b[0m             self.server_url, params={\n\u001b[0;32m     28\u001b[0m                 \u001b[1;34m'properties'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             }, data=data, headers={'Connection': 'close'})\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         if ('outputFormat' in properties\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \"\"\"\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;34m\"\"\"Closes all adapters and as such the session\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[0mwhich\u001b[0m \u001b[0mcloses\u001b[0m \u001b[0many\u001b[0m \u001b[0mpooled\u001b[0m \u001b[0mconnections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \"\"\"\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoolmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mproxy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36mclear\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mre\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mused\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnection_from_host\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\_collections.py\u001b[0m in \u001b[0;36mclear\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispose_func\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispose_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection_pool_kw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection_pool_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         self.pools = RecentlyUsedContainer(num_pools,\n\u001b[1;32m--> 155\u001b[1;33m                                            dispose_func=lambda p: p.close())\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# Locally set the pool classes and keys so other PoolManagers can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# close it manually... there may be other refs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__response\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io_refs\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_real_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36m_real_close\u001b[1;34m(self, _ss)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_real_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;31m# This function should not reference any globals. See issue #808164.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0m_ss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# POS extracted for no preprocessed data\n",
    "unigram_pos, bigrams_pos, trigram_pos = preprocessing.extract_POS(tr_dict['statement'])\n",
    "unigram_pos_va, bigrams_pos_va, trigram_pos_va = preprocessing.extract_POS(statements=va_dict['statement'])\n",
    "\n",
    "# POS without punctuation\n",
    "unigram_pos_no_p, bigrams_pos_no_p, trigram_pos_no_p = preprocessing.extract_POS(tr_data_no_punctuation)\n",
    "unigram_pos_no_p_va, bigrams_pos_no_p_va, trigram_pos_no_p_va = preprocessing.extract_POS(va_data_no_punctuation)\n",
    "\n",
    "# POS withoutn punctuation and with lower case\n",
    "unigram_pos_no_p_u, bigrams_pos_no_p_u, trigram_pos_no_p_u = preprocessing.extract_POS(tr_data_no_punct_upper)\n",
    "unigram_pos_no_p_u_va, bigrams_pos_no_p_u_va, trigram_pos_no_p_u_va = preprocessing.extract_POS(va_data_no_punct_upper)\n",
    "\n",
    "# POW without punctuation, lower case and with no stop words\n",
    "unigram_pos_no_p_u_sw, bigrams_pos_no_p_u_sw, trigram_pos_no_p_u_sw = preprocessing.extract_POS(tr_data_no_punct_upper_stopw)\n",
    "unigram_pos_no_p_u_sw_va, bigrams_pos_no_p_u_sw_va, trigram_pos_no_p_u_sw_va = preprocessing.extract_POS(va_data_no_punct_upper_stopw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivegrams_no_punct = nlp_task.POS_groupping(unigram_pos_no_p, grams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivegrams_no_punct_va = nlp_task.POS_groupping(unigram_pos_no_p_va, grams=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivegrams_list_tr, five_gramcount = nlp_task.UniquePosTags(fivegrams_no_punct, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredFivegrams = [(x) for x in five_gramcount if five_gramcount[x]>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(nlp_util)\n",
    "nlp_task = nlp_util.NLP_Task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique POS by different preprocessing type of text\n",
    "# Unigram of unclean data\n",
    "unigram_list_tr = nlp_task.UniquePosTags(unigram_pos)\n",
    "unigram_list_va = nlp_task.UniquePosTags(unigram_pos_va)\n",
    "\n",
    "# Unigram of no punctuation data\n",
    "unigram_list_tr_no_p = nlp_task.UniquePosTags(unigram_pos_no_p)\n",
    "unigram_list_va_no_p = nlp_task.UniquePosTags(unigram_pos_no_p_va)\n",
    "\n",
    "# Unigram of no punctuation and with lower case\n",
    "unigram_list_tr_no_p_u = nlp_task.UniquePosTags(unigram_pos_no_p_u) \n",
    "unigram_list_va_no_p_u = nlp_task.UniquePosTags(unigram_pos_no_p_u_va)\n",
    "\n",
    "# Unigram of no punctuation, without lower case and without stop words\n",
    "unigram_list_tr_no_p_u_sw = nlp_task.UniquePosTags(unigram_pos_no_p_u_sw)\n",
    "unigram_list_va_no_p_u_sw = nlp_task.UniquePosTags(unigram_pos_no_p_u_sw_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram POS by different preprocessing type of text\n",
    "# Bigram of unclean data\n",
    "bigram_list_tr = nlp_task.UniquePosTags(bigrams_pos)\n",
    "bigram_list_va = nlp_task.UniquePosTags(bigrams_pos_va)\n",
    "\n",
    "# Bigram of no punctuation data\n",
    "bigram_list_tr_no_p = nlp_task.UniquePosTags(bigrams_pos_no_p)\n",
    "bigram_list_va_no_p = nlp_task.UniquePosTags(bigrams_pos_no_p_va)\n",
    "\n",
    "# Bigram of no punctuation and with lower case\n",
    "bigram_list_tr_no_p_u = nlp_task.UniquePosTags(bigrams_pos_no_p_u) \n",
    "bigram_list_va_no_p_u = nlp_task.UniquePosTags(bigrams_pos_no_p_u_va)\n",
    "\n",
    "# Bigram of no punctuation, without lower case and without stop words\n",
    "bigram_list_tr_no_p_u_sw = nlp_task.UniquePosTags(bigrams_pos_no_p_u_sw)\n",
    "bigram_list_va_no_p_u_sw = nlp_task.UniquePosTags(bigrams_pos_no_p_u_sw_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicated POS in bigrams\n",
    "## RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = unigram_list_tr#['NNP','CD']\n",
    "removed_pos = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos)\n",
    "removed_pos_bigrams = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos)\n",
    "\n",
    "removed_pos_va =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_va)\n",
    "removed_pos_bigrams_va = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams = nlp_task.UniquePosTags(postags=removed_pos_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_pos_va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicated POS in trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD','NN']\n",
    "removed_pos_trigrams = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos)\n",
    "removed_pos_trigrams_va = nlp_task.POS_groupping(grams=3,sentences_pos=removed_pos_va)\n",
    "removed_unique_trigrams = nlp_task.UniquePosTags(postags=removed_pos_trigrams_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_unique_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_remove = ['NNP','CD']\n",
    "removed_pos_p = nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_no_p)\n",
    "removed_pos_bigrams_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_p)\n",
    "\n",
    "removed_pos_va_p =  nlp_task.RemoveConsecutiveTags(list_to_remove,unigram_pos_no_p_va)\n",
    "removed_pos_bigrams_va_p = nlp_task.POS_groupping(grams=2,sentences_pos=removed_pos_va_p)\n",
    "\n",
    "#LIST OF UNIQUE BIGRAMS AFTER REMOVING CONSECUTIVE SAME TAGS\n",
    "removed_unique_bigrams_p = nlp_task.UniquePosTags(postags=removed_pos_bigrams_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example: The/DT economy/NN bled/VBD $/$ 24/CD billion/CD due/JJ to/TO the/DT government/NN shutdown/NN ./.\n",
    "# In this case having CD_CD is the same as having only CD\n",
    "# Same with: \n",
    "# U.S./NNP Rep./NNP Ron/NNP Kind/NNP ,/, D-Wis./NNP ,/, and/CC his/PRP$ fellow/JJ Democrats/NNS went/VBD on/IN a/DT spending/NN spree/NN and/CC now/RB their/PRP$ credit/NN card/NN is/VBZ maxed/VBN out/RP\n",
    "# We don't need all those NNPs to find a pattern and it might be noisy to the ML algorithm\n",
    "\n",
    "for i,big in enumerate(removed_pos_bigrams):\n",
    "    hasD = False\n",
    "    for x in big:\n",
    "        if x.split('_')[0]==x.split('_')[1]:\n",
    "            print(x)\n",
    "            hasD=True\n",
    "    if hasD:\n",
    "        print(removed_pos[i])\n",
    "        print(tr_dict['statement'][i])\n",
    "        hasD=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram POS by different preprocessing type of text\n",
    "# Trigram of unclean data\n",
    "trigram_list_tr = nlp_task.UniquePosTags(trigram_pos)\n",
    "trigram_list_va = nlp_task.UniquePosTags(trigram_pos_va)\n",
    "\n",
    "# Trigram of no punctuation data\n",
    "trigram_list_tr_no_p = nlp_task.UniquePosTags(trigram_pos_no_p)\n",
    "trigram_list_va_no_p = nlp_task.UniquePosTags(trigram_pos_no_p_va)\n",
    "\n",
    "# Trigram of no punctuation and with lower case\n",
    "trigram_list_tr_no_p_u = nlp_task.UniquePosTags(trigram_pos_no_p_u) \n",
    "trigram_list_va_no_p_u = nlp_task.UniquePosTags(trigram_pos_no_p_u_va)\n",
    "\n",
    "# Trigram of no punctuation, without lower case and without stop words\n",
    "trigram_list_tr_no_p_u_sw = nlp_task.UniquePosTags(trigram_pos_no_p_u_sw)\n",
    "trigram_list_va_no_p_u_sw = nlp_task.UniquePosTags(trigram_pos_no_p_u_sw_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new labels grouping different classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels_tr = np.array(preprocessing.create_labels(labels=tr_file['Label'].values,label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))\n",
    "binary_labels_va = np.array(preprocessing.create_labels(labels=va_file['Label'].values,label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))\n",
    "binary_labels_te = np.array(preprocessing.create_labels(labels=te_file['Label'].values,label_values={'false':1, 'true':-1,'pants-fire':1,'barely-true':1,'half-true':0,'mostly-true':-1}))\n",
    "\n",
    "tr_indexes = [i for i,x in enumerate(binary_labels_tr) if x!=0]\n",
    "va_indexes = [i for i,x in enumerate(binary_labels_va) if x!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use countVectorizor to build postag output as features for training ML model\n",
    "## uni_vocabulary: vocabulary=['cd','jjr','vb','jjs','nnp','in','vbp','to','rb','vbg','md','jj','dt']\n",
    "## bi_vocabulary: vocabulary=['NNP NNP', 'IN DT', 'JJR IN', 'CD NN', 'IN CD', 'CD NNS', 'DT JJS']\n",
    "## tri_vocabulary: ['NNP NNP NNP','CD NN IN','VBZ NNP NNP','IN DT NN','IN DT JJ','NN IN DT','<s> VBZ NNP','JJR IN CD','NNP NNP VBD','DT JJ CD','NNS IN DT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Train machine learning models with different preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pruned_features_1 = []\n",
    "#pruned_features_2 = []\n",
    "\n",
    "pruned_list = list(np.array(trigram_list_tr_no_p)[pruned_features_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_pos_no_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#training_data = [unigram_pos, unigram_pos_no_p, unigram_pos_no_p_u, unigram_pos_no_p_u_sw] \n",
    "#testing_data = [unigram_pos_va, unigram_pos_no_p_va, unigram_pos_no_p_u_va, unigram_pos_no_p_u_sw_va]\n",
    "#training_data = [bigrams_pos, bigrams_pos_no_p, bigrams_pos_no_p_u, bigrams_pos_no_p_u_sw] \n",
    "#testing_data = [bigrams_pos_va, bigrams_pos_no_p_va, bigrams_pos_no_p_u_va, bigrams_pos_no_p_u_sw_va]\n",
    "training_data = [trigram_pos, trigram_pos_no_p, trigram_pos_no_p_u, trigram_pos_no_p_u_sw] \n",
    "testing_data = [trigram_pos_va, trigram_pos_no_p_va, trigram_pos_no_p_u_va, trigram_pos_no_p_u_sw_va]\n",
    "testing_title = ['RAW DATA', 'NO PUNCTUATION', 'NO PUNCTUATION LOWER CASE', 'NO PUNCTUATION LC STOP WORDS']\n",
    "\n",
    "#trda = [fivegrams_no_punct]#training_data[1:2]\n",
    "trda = [removed_pos_bigrams]\n",
    "#teda = [fivegrams_no_punct_va]#testing_data[1:2]\n",
    "teda = [removed_pos_bigrams_va]\n",
    "tetil = testing_title[0:1]\n",
    "\n",
    "for i, each_data in enumerate(trda):\n",
    "    print(\"TRAINING POSTags:\", tetil[i])\n",
    "    print(\"Testing with user defined vectors\")\n",
    "    Xtr, Xte = PreparePOSDataForTraining(each_data, teda[i], use_built_in_vectors=False,user_defined_vocabulary=removed_unique_bigrams, binary=True)\n",
    "    print(len(Xtr))\n",
    "    print(len(Xte))\n",
    "    Xtr = Xtr[tr_indexes]\n",
    "    Xte = Xte[va_indexes]\n",
    "    Ytr = binary_labels_tr[tr_indexes]\n",
    "    Yte = binary_labels_va[va_indexes]\n",
    "    FeatureSelector(Xtr, Xte, Ytr, Yte, removed_unique_bigrams,max_depth=2, threshold=0.000001)\n",
    "    #print(pruned_features_1)\n",
    "    #DecisionTreeFeaturesSelector(Xtr, Xte, Ytr, Yte)\n",
    "    #ExtraTreeFeaturesSelector(Xtr, Xte, Ytr, Yte)\n",
    "    #TrainModels(Xtr, Xte, Ytr, Yte)\n",
    "    print(\"Testing with sklearn built in vectors\")\n",
    "    Xtr, Xte = PreparePOSDataForTraining(each_data, teda[i], use_built_in_vectors=True,user_defined_vocabulary=removed_unique_bigrams, binary=True)\n",
    "    Xtr = Xtr[tr_indexes]\n",
    "    Xte = Xte[va_indexes]\n",
    "    Ytr = binary_labels_tr[tr_indexes]\n",
    "    Yte = binary_labels_va[va_indexes]\n",
    "    FeatureSelector(Xtr, Xte, Ytr, Yte, removed_unique_bigrams,max_depth=4,threshold=0.000001)\n",
    "    #DecisionTreeFeaturesSelector(Xtr, Xte, Ytr, Yte)\n",
    "    #ExtraTreeFeaturesSelector(Xtr, Xte, Ytr, Yte)\n",
    "    #TrainModels(Xtr, Xte, Ytr, Yte)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(trigram_list_tr_no_p)[pruned_features_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeFeaturesSelector(Xtr, Xte, Ytr, Yte):\n",
    "    for i in range(1,10):\n",
    "        print('max_depth',i)\n",
    "        clf = DecisionTreeClassifier(max_depth=i)\n",
    "        clf.fit(Xtr, Ytr)\n",
    "        pred = clf.predict(Xte)\n",
    "        print('Accuracy: ', np.mean(Yte == pred))\n",
    "        print(classification_report(Yte, pred))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtraTreeFeaturesSelector(Xtr, Xte, Ytr, Yte):\n",
    "    for i in range(1,10):\n",
    "        print('Estimators',i)\n",
    "        clf = ExtraTreesClassifier(n_estimators=i, max_depth=None,min_samples_split=2, random_state=0)\n",
    "        clf.fit(Xtr, Ytr)\n",
    "        pred = clf.predict(Xte)\n",
    "        print('Accuracy: ', np.mean(Yte == pred))\n",
    "        print(classification_report(Yte, pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureSelector(Xtr, Xte, Ytr, Yte,feature_list,max_depth = 1,threshold=0.1):\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    clf.fit(Xtr, Ytr)\n",
    "    pred = clf.predict(Xte)\n",
    "    print('Accuracy: ', np.mean(Yte == pred))\n",
    "    print(classification_report(Yte, pred)) \n",
    "    \n",
    "    pos_Tags = list()\n",
    "    for i,each_f in enumerate(clf.feature_importances_):\n",
    "        if each_f >=threshold:\n",
    "            pos_Tags.append(feature_list[i])\n",
    "            print(feature_list[i], each_f)\n",
    "    print(pos_Tags)\n",
    "    return (clf.feature_importances_<=threshold) # return filetered features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_built_in_vectors => if True use CountVectorizer, otherwise it uses the version of pos_vectors in preprocessing\n",
    "def PreparePOSDataForTraining(training_data, testing_data, use_built_in_vectors = False, user_defined_vocabulary=None, binary=True):\n",
    "    tr_feats = []\n",
    "    te_feats = []\n",
    "    if(use_built_in_vectors == False):\n",
    "        tr_feats, returned_dict = preprocessing.pos_vectors(training_data, vector_dictionary= user_defined_vocabulary, return_dictionary=True)\n",
    "        if binary:\n",
    "            te_feats = preprocessing.pos_vectors(testing_data, vector_dictionary= returned_dict)\n",
    "        else:\n",
    "            binary_count, te_feats = preprocessing.pos_vectors(testing_data, vector_dictionary= returned_dict, count=True)\n",
    "    else:\n",
    "        if user_defined_vocabulary != None:\n",
    "            user_defined_vocabulary = [x.lower() for x in user_defined_vocabulary]\n",
    "        training_str = [\" \".join(x) for x in training_data]\n",
    "        testing_str = [\" \".join(x) for x in testing_data]\n",
    "        tr_vectorizer = CountVectorizer(vocabulary=user_defined_vocabulary,binary=binary)\n",
    "        tr_feats = tr_vectorizer.fit_transform(training_str).toarray()\n",
    "        te_vectorizer = CountVectorizer(vocabulary=tr_vectorizer.get_feature_names(),binary=binary)\n",
    "        te_feats = te_vectorizer.fit_transform(testing_str).toarray()\n",
    "    return tr_feats, te_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Different Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModels(Xtrain, Xtest, Ytrain, Ytest):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    # ,'class_weight':{1:.9, 2:.5, 3:.01}\n",
    "    params = [{'max_depth':1,'criterion':'entropy'},{}, {'loss': 'log', 'penalty': 'l2', 'max_iter':1000},{},{},{}]\n",
    "    Models = [DecisionTreeClassifier,LogisticRegression, linear_model.SGDClassifier, MultinomialNB, RandomForestClassifier, SVC]\n",
    "    accuracy_list = list()\n",
    "    accuracy_metrics = list()\n",
    "    for param, Model in zip(params, Models):\n",
    "        total = 0\n",
    "        for train_indices, test_indices in kf.split(Xtrain):\n",
    "            train_X = Xtrain[train_indices, :]; train_Y = Ytrain[train_indices]\n",
    "            test_X = Xtrain[test_indices, :]; test_Y = Ytrain[test_indices]\n",
    "            reg = Model(**param)\n",
    "            reg.fit(train_X, train_Y)\n",
    "            predictions = reg.predict(test_X)\n",
    "            total += accuracy_score(test_Y, predictions)\n",
    "        accuracy = total / kf.n_splits\n",
    "        reg = Model(**param)\n",
    "        reg.fit(Xtrain, Ytrain)\n",
    "        predictions = reg.predict(Xtest)\n",
    "        accuracy_list.append((Model.__name__, accuracy))\n",
    "        accuracy_metrics.append((Model.__name__, classification_report(Ytest, predictions)))\n",
    "\n",
    "    for i, value in enumerate(accuracy_list):\n",
    "        print(\"Accuracy score of {0}: {1}\".format(value[0],value[1]))\n",
    "        print(\"accuracy metrics for {0}:\\n{1}\".format(accuracy_metrics[i][0], accuracy_metrics[i][1]))\n",
    "\n",
    "#print(\"Accuracy score of {0}: {1}: {2}: {3}: {4}\".format(accuracy_list[0], accuracy_list[]))\n",
    "#print(\"accuracy metrics for {0}: {1}: {2}: {3}: {4}\".format(Model.__name__, classification_report(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
